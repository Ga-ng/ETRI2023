{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98081ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb680fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels:int=3, patch_size:int=16, emb_size:int=768, img_size:int=224):\n",
    "        self.P = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e h w -> b (h w) e')\n",
    "        )\n",
    "        self.cls_token= nn.Parameter(torch.randn(1,1,emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_size))\n",
    "    def forward(self, x:Tensor)->Tensor:\n",
    "        b,_,_,_ = x.shape\n",
    "        x =self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x+= self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca01a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "    def __init__(self, emb_size:int=768, num_heads:int=8, dropout:float=0):\n",
    "        super().__init__()\n",
    "        self.emb_size= emb_size\n",
    "        self.num_heads = num_heads\n",
    "        #Fuse the queries, keys, values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size*3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x:Tensor, mask:Tensor = None)-> Tensor:\n",
    "        #split keys, queries, and vlaues in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h = self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        #sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) #batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.emb_size**(1/2)\n",
    "        att = F.softmax(energy, dim =-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        \n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f8d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self,fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x+= res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17d39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self,emb_size:int, expansion : int=4, drop_p :float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion* emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion*emb_size, emb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f27956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                emb_size:int=768,\n",
    "                drop_p : float=0,\n",
    "                forward_expansion : int=4,\n",
    "                forward_drop_p : float=0.,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion,\n",
    "                    drop_p = forward_drop_p),\n",
    "                )\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadf05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth:int =12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9966f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size:int =768, n_classes: int=10):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction ='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                in_channels: int=3,\n",
    "                patch_size : int=16,\n",
    "                emb_size :int=768,\n",
    "                img_size:int=12,\n",
    "                depth: int=12,\n",
    "                n_classes:int =10,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "        TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "        ClassificationHead(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9dcd99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-16             [-1, 197, 768]               0\n",
      "        LayerNorm-17             [-1, 197, 768]           1,536\n",
      "           Linear-18            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-19          [-1, 8, 197, 197]               0\n",
      "           Linear-20             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-21             [-1, 197, 768]               0\n",
      "          Dropout-22             [-1, 197, 768]               0\n",
      "      ResidualAdd-23             [-1, 197, 768]               0\n",
      "        LayerNorm-24             [-1, 197, 768]           1,536\n",
      "           Linear-25            [-1, 197, 3072]       2,362,368\n",
      "             GELU-26            [-1, 197, 3072]               0\n",
      "          Dropout-27            [-1, 197, 3072]               0\n",
      "           Linear-28             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-29             [-1, 197, 768]               0\n",
      "        LayerNorm-30             [-1, 197, 768]           1,536\n",
      "           Linear-31            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-32          [-1, 8, 197, 197]               0\n",
      "           Linear-33             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-34             [-1, 197, 768]               0\n",
      "          Dropout-35             [-1, 197, 768]               0\n",
      "      ResidualAdd-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 3072]       2,362,368\n",
      "             GELU-39            [-1, 197, 3072]               0\n",
      "          Dropout-40            [-1, 197, 3072]               0\n",
      "           Linear-41             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-42             [-1, 197, 768]               0\n",
      "        LayerNorm-43             [-1, 197, 768]           1,536\n",
      "           Linear-44            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-45          [-1, 8, 197, 197]               0\n",
      "           Linear-46             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-47             [-1, 197, 768]               0\n",
      "          Dropout-48             [-1, 197, 768]               0\n",
      "      ResidualAdd-49             [-1, 197, 768]               0\n",
      "        LayerNorm-50             [-1, 197, 768]           1,536\n",
      "           Linear-51            [-1, 197, 3072]       2,362,368\n",
      "             GELU-52            [-1, 197, 3072]               0\n",
      "          Dropout-53            [-1, 197, 3072]               0\n",
      "           Linear-54             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-55             [-1, 197, 768]               0\n",
      "        LayerNorm-56             [-1, 197, 768]           1,536\n",
      "           Linear-57            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-58          [-1, 8, 197, 197]               0\n",
      "           Linear-59             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-60             [-1, 197, 768]               0\n",
      "          Dropout-61             [-1, 197, 768]               0\n",
      "      ResidualAdd-62             [-1, 197, 768]               0\n",
      "        LayerNorm-63             [-1, 197, 768]           1,536\n",
      "           Linear-64            [-1, 197, 3072]       2,362,368\n",
      "             GELU-65            [-1, 197, 3072]               0\n",
      "          Dropout-66            [-1, 197, 3072]               0\n",
      "           Linear-67             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-71          [-1, 8, 197, 197]               0\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-73             [-1, 197, 768]               0\n",
      "          Dropout-74             [-1, 197, 768]               0\n",
      "      ResidualAdd-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "           Linear-80             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-81             [-1, 197, 768]               0\n",
      "        LayerNorm-82             [-1, 197, 768]           1,536\n",
      "           Linear-83            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-84          [-1, 8, 197, 197]               0\n",
      "           Linear-85             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-86             [-1, 197, 768]               0\n",
      "          Dropout-87             [-1, 197, 768]               0\n",
      "      ResidualAdd-88             [-1, 197, 768]               0\n",
      "        LayerNorm-89             [-1, 197, 768]           1,536\n",
      "           Linear-90            [-1, 197, 3072]       2,362,368\n",
      "             GELU-91            [-1, 197, 3072]               0\n",
      "          Dropout-92            [-1, 197, 3072]               0\n",
      "           Linear-93             [-1, 197, 768]       2,360,064\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-97          [-1, 8, 197, 197]               0\n",
      "           Linear-98             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-99             [-1, 197, 768]               0\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 3072]       2,362,368\n",
      "            GELU-104            [-1, 197, 3072]               0\n",
      "         Dropout-105            [-1, 197, 3072]               0\n",
      "          Linear-106             [-1, 197, 768]       2,360,064\n",
      "     ResidualAdd-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-110          [-1, 8, 197, 197]               0\n",
      "          Linear-111             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-112             [-1, 197, 768]               0\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "     ResidualAdd-114             [-1, 197, 768]               0\n",
      "       LayerNorm-115             [-1, 197, 768]           1,536\n",
      "          Linear-116            [-1, 197, 3072]       2,362,368\n",
      "            GELU-117            [-1, 197, 3072]               0\n",
      "         Dropout-118            [-1, 197, 3072]               0\n",
      "          Linear-119             [-1, 197, 768]       2,360,064\n",
      "     ResidualAdd-120             [-1, 197, 768]               0\n",
      "       LayerNorm-121             [-1, 197, 768]           1,536\n",
      "          Linear-122            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-123          [-1, 8, 197, 197]               0\n",
      "          Linear-124             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-125             [-1, 197, 768]               0\n",
      "         Dropout-126             [-1, 197, 768]               0\n",
      "     ResidualAdd-127             [-1, 197, 768]               0\n",
      "       LayerNorm-128             [-1, 197, 768]           1,536\n",
      "          Linear-129            [-1, 197, 3072]       2,362,368\n",
      "            GELU-130            [-1, 197, 3072]               0\n",
      "         Dropout-131            [-1, 197, 3072]               0\n",
      "          Linear-132             [-1, 197, 768]       2,360,064\n",
      "     ResidualAdd-133             [-1, 197, 768]               0\n",
      "       LayerNorm-134             [-1, 197, 768]           1,536\n",
      "          Linear-135            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-136          [-1, 8, 197, 197]               0\n",
      "          Linear-137             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-138             [-1, 197, 768]               0\n",
      "         Dropout-139             [-1, 197, 768]               0\n",
      "     ResidualAdd-140             [-1, 197, 768]               0\n",
      "       LayerNorm-141             [-1, 197, 768]           1,536\n",
      "          Linear-142            [-1, 197, 3072]       2,362,368\n",
      "            GELU-143            [-1, 197, 3072]               0\n",
      "         Dropout-144            [-1, 197, 3072]               0\n",
      "          Linear-145             [-1, 197, 768]       2,360,064\n",
      "     ResidualAdd-146             [-1, 197, 768]               0\n",
      "       LayerNorm-147             [-1, 197, 768]           1,536\n",
      "          Linear-148            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-149          [-1, 8, 197, 197]               0\n",
      "          Linear-150             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-151             [-1, 197, 768]               0\n",
      "         Dropout-152             [-1, 197, 768]               0\n",
      "     ResidualAdd-153             [-1, 197, 768]               0\n",
      "       LayerNorm-154             [-1, 197, 768]           1,536\n",
      "          Linear-155            [-1, 197, 3072]       2,362,368\n",
      "            GELU-156            [-1, 197, 3072]               0\n",
      "         Dropout-157            [-1, 197, 3072]               0\n",
      "          Linear-158             [-1, 197, 768]       2,360,064\n",
      "     ResidualAdd-159             [-1, 197, 768]               0\n",
      "          Reduce-160                  [-1, 768]               0\n",
      "       LayerNorm-161                  [-1, 768]           1,536\n",
      "          Linear-162                   [-1, 10]           7,690\n",
      "================================================================\n",
      "Total params: 85,654,282\n",
      "Trainable params: 85,654,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 350.47\n",
      "Params size (MB): 326.75\n",
      "Estimated Total Size (MB): 677.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = ViT()\n",
    "net.to('cuda')\n",
    "\n",
    "summary(net, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7568fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
